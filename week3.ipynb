{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-09T18:08:34.384063Z","iopub.execute_input":"2026-02-09T18:08:34.384398Z","iopub.status.idle":"2026-02-09T18:08:35.592098Z","shell.execute_reply.started":"2026-02-09T18:08:34.384362Z","shell.execute_reply":"2026-02-09T18:08:35.591201Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# MLP for XOR – Effect of Learning Rate on Loss\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# --------------------------------------------------\n# Activation functions\n# --------------------------------------------------\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef sigmoid_derivative(x):\n    return x * (1 - x)\n\n# --------------------------------------------------\n# XOR Dataset\n# --------------------------------------------------\nX = np.array([\n    [0, 0],\n    [0, 1],\n    [1, 0],\n    [1, 1]\n])\n\ny = np.array([[0], [1], [1], [0]])\n\n# --------------------------------------------------\n# Learning rates to test\n# --------------------------------------------------\nlearning_rates = [0.01, 0.05, 0.1, 0.5, 1.0]\nfinal_losses = []\n\nepochs = 10000\n\n# --------------------------------------------------\n# Train model for each learning rate\n# --------------------------------------------------\nfor lr in learning_rates:\n\n    np.random.seed(42)  # Same initialization for fair comparison\n\n    # Initialize weights and biases\n    W1 = np.random.rand(2, 2)\n    b1 = np.random.rand(1, 2)\n\n    W2 = np.random.rand(2, 1)\n    b2 = np.random.rand(1, 1)\n\n    # Training loop\n    for epoch in range(epochs):\n\n        # Forward propagation\n        hidden_input = np.dot(X, W1) + b1\n        hidden_output = sigmoid(hidden_input)\n\n        final_input = np.dot(hidden_output, W2) + b2\n        y_pred = sigmoid(final_input)\n\n        # Loss\n        error = y - y_pred\n        loss = np.mean(error ** 2)\n\n        # Backpropagation\n        d_output = error * sigmoid_derivative(y_pred)\n        d_hidden = d_output.dot(W2.T) * sigmoid_derivative(hidden_output)\n\n        # Update weights and biases\n        W2 += hidden_output.T.dot(d_output) * lr\n        b2 += np.sum(d_output, axis=0, keepdims=True) * lr\n\n        W1 += X.T.dot(d_hidden) * lr\n        b1 += np.sum(d_hidden, axis=0, keepdims=True) * lr\n\n    final_losses.append(loss)\n    print(f\"Learning Rate: {lr}, Final Loss: {loss:.6f}\")\n\nprint(y_pred)\nprint(error)\nprint(loss)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T18:43:23.405784Z","iopub.execute_input":"2026-02-09T18:43:23.406166Z","iopub.status.idle":"2026-02-09T18:43:26.701416Z","shell.execute_reply.started":"2026-02-09T18:43:23.406137Z","shell.execute_reply":"2026-02-09T18:43:26.700441Z"}},"outputs":[{"name":"stdout","text":"Learning Rate: 0.01, Final Loss: 0.240571\nLearning Rate: 0.05, Final Loss: 0.012541\nLearning Rate: 0.1, Final Loss: 0.002546\nLearning Rate: 0.5, Final Loss: 0.000293\nLearning Rate: 1.0, Final Loss: 0.000135\n[[0.01294094]\n [0.9889538 ]\n [0.98894283]\n [0.01136249]]\n[[-0.01294094]\n [ 0.0110462 ]\n [ 0.01105717]\n [-0.01136249]]\n0.0001352134372827767\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from sklearn.linear_model import Perceptron\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Load the iris dataset\niris = load_iris()\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=0)\n\n# Create a Perceptron object\nperceptron = Perceptron()\n\n# Train the Perceptron on the training data\nperceptron.fit(X_train, y_train)\n\n# Use the trained Perceptron to make predictions on the testing data\ny_pred = perceptron.predict(X_test)\n\n# Evaluate the accuracy of the Perceptron\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T18:43:56.897804Z","iopub.execute_input":"2026-02-09T18:43:56.898172Z","iopub.status.idle":"2026-02-09T18:43:57.109649Z","shell.execute_reply.started":"2026-02-09T18:43:56.898141Z","shell.execute_reply":"2026-02-09T18:43:57.108855Z"}},"outputs":[{"name":"stdout","text":"Accuracy: 0.8\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from tensorflow.keras.datasets import mnist\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPClassifier\n\n# Load MNIST\n(X, y), _ = mnist.load_data()\n\n# Normalize & flatten\nX = X / 255.0\nX = X.reshape(-1, 28*28)\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.1, random_state=42\n)\n\n# MLP with default learning rate\nmlp1 = MLPClassifier(hidden_layer_sizes=(50,), max_iter=50, random_state=1)\nmlp1.fit(X_train, y_train)\n\nprint(\"MLP1 Train Accuracy:\", mlp1.score(X_train, y_train))\nprint(\"MLP1 Test Accuracy :\", mlp1.score(X_test, y_test))\n\n# MLP with higher learning rate\nmlp2 = MLPClassifier(hidden_layer_sizes=(50,),\n                     max_iter=50,\n                     learning_rate_init=0.1,\n                     random_state=1)\nmlp2.fit(X_train, y_train)\n\nprint(\"MLP2 Train Accuracy:\", mlp2.score(X_train, y_train))\nprint(\"MLP2 Test Accuracy :\", mlp2.score(X_test, y_test))\n\n# Test one sample\nidx = 100\nprint(\"Predicted:\", mlp2.predict(X_test[idx].reshape(1, -1))[0])\nprint(\"Actual   :\", y_test[idx])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T18:36:34.580230Z","iopub.execute_input":"2026-02-09T18:36:34.580567Z","iopub.status.idle":"2026-02-09T18:37:52.240274Z","shell.execute_reply.started":"2026-02-09T18:36:34.580536Z","shell.execute_reply":"2026-02-09T18:37:52.236762Z"}},"outputs":[{"name":"stderr","text":"2026-02-09 18:36:36.398280: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1770662196.607033      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1770662196.664648      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1770662197.144353      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770662197.144395      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770662197.144399      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770662197.144402      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"MLP1 Train Accuracy: 0.9996666666666667\nMLP1 Test Accuracy : 0.974\nMLP2 Train Accuracy: 0.8826481481481482\nMLP2 Test Accuracy : 0.8745\nPredicted: 2\nActual   : 2\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from tensorflow.keras.datasets import mnist\nfrom sklearn.neural_network import MLPClassifier\n\n# Load & preprocess\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\nX_train = X_train.reshape(-1, 784) / 255.0\nX_test  = X_test.reshape(-1, 784) / 255.0\n\n# MLP model\nmlp = MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=20, random_state=1)\n\n# Train & test\nmlp.fit(X_train, y_train)\nprint(\"Accuracy:\", mlp.score(X_test, y_test))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T18:42:38.768571Z","iopub.execute_input":"2026-02-09T18:42:38.769851Z","iopub.status.idle":"2026-02-09T18:43:05.165838Z","shell.execute_reply.started":"2026-02-09T18:42:38.769806Z","shell.execute_reply":"2026-02-09T18:43:05.165066Z"}},"outputs":[{"name":"stdout","text":"Accuracy: 0.974\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":3}]}